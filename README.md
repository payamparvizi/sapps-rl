# SAPPS-RL
## Adaptive Policy Regularization for Smooth Control in Reinforcement Learning

This repository contains the official implementation of **State-Adaptive Proportional Policy Smoothing (SAPPS)**, a policy regularization method designed to produce **smooth yet responsive control policies** in continuous-control reinforcement learning.

SAPPS suppresses high-frequency oscillations in learned policies **without compromising performance**, particularly in **dynamic environments** where rapid adaptation is required.

ðŸ“„ **Paper**: *Adaptive Policy Regularization for Smooth Control in Reinforcement Learning*  
ðŸ“Œ **Status**: Under review  
ðŸ‘¤ **Authors**: Payam Parvizi, Abhishek Naik, Colin Bellinger, Ross Cheriton, Davide Spinello  
ðŸ”— **Repository**: https://github.com/payamparvizi/sapps-rl  

---

## Abstract

A significant challenge in applying reinforcement learning (RL) to continuous-control problems is the presence of high-frequency oscillations in the actions produced by learned policies. These oscillations result in abrupt control responses, leading to excessive actuator wear, increased power consumption, and instability in real-world deployments. Existing approaches designed to reduce such oscillations often involve trade-offs, including increased architectural complexity or degraded policy performance, particularly in environments where rapid state changes require rapid adaptation.

To address this issue, we propose **State-Adaptive Proportional Policy Smoothing (SAPPS)**, a novel approach that adaptively adjusts smoothness constraints to suppress high-frequency components in RL policies. SAPPS is inspired by Lipschitz continuity and introduces a state-adaptive proportional regularization during policy optimization, encouraging changes in consecutive actions to scale with changes in consecutive observations. This enables smooth yet responsive control.

Results from simulation and hardware experiments demonstrate that SAPPS produces smooth control policies without compromising performance across a diverse set of environments, including MuJoCo continuous-control benchmarks, a simulated adaptive optics system for optical satellite communications, and a real-world nano quadcopter, under both slowly and rapidly changing conditions.

---

## Method Overview

SAPPS is a general policy regularization method that can be integrated into deep RL algorithms to improve policy smoothness in both static and dynamic continuous-control environments. Rather than directly penalizing action magnitude or velocity, SAPPS regularizes changes between consecutive actions based on the relative magnitude of variation between temporally consecutive observations.

This adaptive formulation:
- penalizes unnecessary action fluctuations when state changes are small, and
- preserves responsiveness when large observation changes require rapid control adaptation.

SAPPS is implemented within **Proximal Policy Optimization (PPO)** and compared against:
- Vanilla PPO  
- PPO with Conditioning for Action Policy Smoothness (CAPS)  
- PPO with LipsNet-based architectures  

---

## Repository Structure

```
sapps-rl/
â”œâ”€â”€ Adaptive_Optics_Environment/
â”‚   â””â”€â”€ RL environment for wavefront sensorless adaptive optics
â”‚
â”œâ”€â”€ MuJoCo_Environments/
â”‚   â””â”€â”€ Standard MuJoCo continuous-control benchmark tasks
â”‚
â”œâ”€â”€ Quadcopter_Environment/
â”‚   â””â”€â”€ Real-world nano quadcopter hovering experiments
â”‚
â””â”€â”€ README.md
```

Each environment directory is self-contained and includes training and evaluation scripts corresponding to the experiments reported in the paper.

---

## Experimental Domains

### 1. MuJoCo Continuous-Control Benchmarks
SAPPS is evaluated on standard OpenAI Gymnasium MuJoCo tasks, including:
- Walker2D  
- HalfCheetah  
- Ant  
- Reacher  
- Swimmer  

Across these benchmarks, SAPPS improves policy smoothness while maintaining or improving task return.

### 2. Real-World Quadcopter Control
A nano quadcopter hovering task is used to validate real-world applicability. SAPPS demonstrates reduced control oscillations, improved actuator efficiency, and stable performance under sensor noise and external disturbances.

### 3. Wavefront Sensorless Adaptive Optics
A highly dynamic optical control problem inspired by satellite-to-ground optical communication. SAPPS is evaluated under quasi-static atmospheric conditions and rapidly changing turbulence with high drift velocities, maintaining performance where fixed smoothing methods degrade.

---

## Installation

This repository assumes a standard Python-based reinforcement learning stack.

### Requirements
- Python â‰¥ 3.8  
- PyTorch  
- NumPy  
- OpenAI Gymnasium / MuJoCo  

Additional dependencies may be required depending on the selected environment and are specified within each subdirectory.

---

## Running Experiments

Each environment directory contains its own training and evaluation scripts. Please refer to environment-specific scripts and configuration files for details on hyperparameters and experimental settings.

---

## Reproducibility

All reported results are averaged over multiple random seeds. Hyperparameter ranges match those described in the paper, and evaluation protocols are consistent across all compared methods. Due to simulator versions, hardware differences, and stochasticity, exact numerical reproduction may vary; however, qualitative trends are robust.

---

## Citation

If you use this code in your research, please cite:

```bibtex
@article{parvizi2025sapps,
  title={Adaptive Policy Regularization for Smooth Control in Reinforcement Learning},
  author={Parvizi, Payam and Naik, Abhishek and Bellinger, Colin and Cheriton, Ross and Spinello, Davide},
  journal={IEEE Transactions on Automation Science and Engineering},
  year={2025},
  note={under review}
}
```

A `CITATION.cff` file will be added upon publication.

---

## License

This repository will be released under an open-source license upon publication.

---

## Contact

**Payam Parvizi**  
Email: pparv056@uottawa.ca  
GitHub: https://github.com/payamparvizi

---

## Acknowledgments

This work was supported in part by the **Natural Sciences and Engineering Research Council of Canada (NSERC)** and in part by the **National Research Council Canada (NRC)**.
